[
  {
    "objectID": "posts/Blog5/index.html",
    "href": "posts/Blog5/index.html",
    "title": "Blog 5 : Anomaly/Outliers Detection",
    "section": "",
    "text": "In the intricate landscape of data analysis, outliers emerge as the unconventional elements that refuse to conform. These peculiar data points, standing out from the crowd, can introduce significant distortions to the overall dataset. Whether stemming from errors, inconsistencies, or simply unique observations, managing outliers becomes a pivotal step in the process of data cleaning and preprocessing. This careful curation ensures that our analytical endeavors are built upon trustworthy and representative data.\nThe Significance of Outlier Detection:\nThink of outlier detection as playing detective with our dataset. During the crucial phases of cleaning and preprocessing, where we address missing values and pinpoint outliers, it’s imperative to contextualize our actions. The decision to remove outliers should be influenced by the specific use case, as blindly discarding data points may not always align with the broader objectives. The ultimate aim is to bolster our model’s performance when faced with new or unseen data. \nStrategies for Outlier Detection:\n\n\nUsing Standard Deviation: An approach grounded in setting limits based on standard deviations. Lower Limit: μ - 3σ, Upper Limit: μ + 3σ. Any data point beyond this range is flagged as an outlier. This method relies on the assumption that 99.7% of data falls within three standard deviations.\nUsing Z-score: Standard deviation is a metric of variance, i.e., how much the individual data points are spread out from the mean. In statistics, if a data distribution is approximately normal, then about 68% of the data values lie within one standard deviation of the mean and about 95% are within two standard deviations, and about 99.7% lie within three standard deviations. The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. The intuition behind the Z-score is to describe any data point by finding its relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1, i.e., normal distribution. You must be wondering how this helps in identifying the outliers. While calculating the Z-score, we re-scale and center the data and look for data points that are too far from zero. These data points, which are way too far from zero, will be treated as the outliers. In most cases, a threshold of 3 or -3 is used, i.e., if the Z-score value is greater than or less than 3 or -3, respectively, that data point will be identified as an outlier. This technique assumes a Gaussian distribution of the data. The outliers are the data points that are in the tails of the distribution and therefore far from the mean. How far depends on a set threshold for the normalized data points calculated with the formula:\n\nAn outlier is then a normalized data point that has an absolute value greater than z(thr). That is |z(score)| &gt; z(thr). Commonly used Z(thr) values are 2.5, 3.0, and 3.5. Here we will be using 3.0. For example, I’ll take up the Medical Cost Personal Datasets for explaining the Z-Score method.This technique assumes a Gaussian distribution of the data.\n\nUsing IQR (Interquartile Range):\n\nThe concept of the Interquartile Range (IQR) is used to build the boxplot graphs. IQR is a concept in statistics that is used to measure the statistical dispersion and data variability by dividing the dataset into quartiles.In simple words, any dataset or any set of observations is divided into four defined intervals based upon the values of the data and how they compare to the entire dataset. A quartile is what divides the data into three points and four intervals.It is the difference between the third quartile and the first quartile (IQR = Q3 - Q1). Outliers in this case are defined as the observations that are below (Q1−1.5×IQR) or above (Q3+1.5×IQR) or boxplot lower whisker or above boxplot upper whisker. It can be visually represented by the box plot.\nFocused on the central 50% of data. Outliers are identified outside the range[q25−1.5×IQR,q75+1.5×IQR]\n https://www.kdnuggets.com/2019/11/understanding-boxplots.html\n\nUsing Percentile:Leveraging percentiles to isolate outliers outside the interquartile range.Outliers are discarded if they stray beyond the confines of [q25−1.5×IQR,q75+1.5×IQR]. In the intricate dance of data analysis, the ability to detect and navigate outliers is akin to mastering a subtle art. Employing techniques such as standard deviation, z-score, IQR, and percentiles empowers us to pinpoint and manage anomalies with finesse. The choice of method is nuanced, guided by the unique characteristics of the dataset and the specific demands of the analysis. By embracing the challenge of outlier detection, we fortify our data’s integrity, ensuring it stands resilient and poised for insightful revelations. More Content: Outliers can have many causes, such as: Measurement or input error. Data corruption. True outlier observation. There is no precise way to define and identify outliers in general because of the specifics of each dataset. Instead, you, or a domain expert, must interpret the raw observations and decide whether a value is an outlier or not. Nevertheless, we can use statistical methods to identify observations that appear to be rare or unlikely given the available data. This does not mean that the values identified are outliers and should be removed. A good tip is to consider plotting the identified outlier values, perhaps in the context of non-outlier values to see if there are any systematic relationships or patterns to the outliers.\n\nTypes of Outliers:\nOutliers can be of two types:\n\nUnivariate\nMultivariate.\n\nUnivariate outliers can be found when we look at the distribution of a single variable. Multivariate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions. Let us understand this with an example. Let’s say we are understanding the relationship between height and weight. Below, we have univariate and bivariate distribution for Height and Weight. Take a look at the box plot. We do not have any outliers (above and below 1.5 * IQR, a common method). Now look at the scatter plot. Here, we have two values below and one above the average in a specific segment of weight and height.\nImpact of Outliers on a Dataset: Outliers can drastically change the results of data analysis and statistical modeling. There are numerous unfavorable impacts of outliers in the dataset: It increases the error variance and reduces the power of statistical tests. If the outliers are non-randomly distributed, they can decrease normality. They can bias or influence estimates that may be of substantive interest. They can also impact the basic assumption of Regression, ANOVA, and other statistical model assumptions. To understand the impact deeply, let’s take an example to check what happens to a dataset with and without outliers in the dataset.\nCode : For percentile 1. Import all the libraries\n\nLoad and clean the dataset\nFinding the percentile\nVisualization\n\nFor Standard Deviation Download the dataset from : https://www.kaggle.com/datasets/spscientist/students-performance-in-exams\n\nImport libraries\nRead the dataset\nStandard deviation\nVisualization"
  },
  {
    "objectID": "posts/Blog3/index.html",
    "href": "posts/Blog3/index.html",
    "title": "Blog 3 : Linear and nonlinear regression",
    "section": "",
    "text": "Introduction: In the dynamic world of Machine Learning, where predictions and decisions guide intelligent systems, Regression emerges as a powerful technique for understanding relationships between variables. In this blog, we embark on a journey through the realms of Linear and Non-Linear Regression, exploring their nuances, applications, and the impact they can have on predicting future outcomes. Linear Models: Linear models are characterized by having parameters in linear positions within the equation. The general form of a linear model is expressed as y=a+bx. Here, y is the dependent variable, x is the independent variable, and a and b are the parameters. Linear models can also include additional terms such as cx^2 or b/x, as long as the parameters are in linear positions. Examples of Linear Models: y = a+bx line y = a + bx + cx^2 y = a + (b/x) Let’s start coding :\nCODE\nNonlinear Models: Nonlinear models are characterized by having parameters in nonlinear positions within the equation. The general form of a nonlinear model is expressed as : y=f(X,β)+ε where f is a nonlinear function of the independent variable(s) X and parameters β. Nonlinear models capture more complex relationships, allowing for curves, exponentials, logarithms, and other nonlinear patterns. Examples of Nonlinear Models: 1. y = zx^b 2. y = ae^(bx) 3. y = ae^(k/x)\nCODE\nPlotting the Dataset This is what the datapoints look like. It kind of looks like an either logistic or exponential function. The growth starts off slow, then from 2005 on forward, the growth is very significant. And finally, it decelerate slightly in the 2010s.\nCODE\nChoosing a model From an initial look at the plot, we determine that the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:\nThe formula for the logistic function is the following: IMAGE β1: Controls the curve’s steepness, β2: Slides the curve on the x-axis. Building The Model Now, let’s build our regression model and initialize its parameters.\nCODE\nLets look at a sample sigmoid line that might fit with the data:\nCODE\nOur task here is to find the best parameters for our model. Lets first normalize our x and y:\nCODE\nHow we find the best parameters for our fit line? we can use curve_fit which uses non-linear least squares to fit our sigmoid function, to data. Optimal values for the parameters so that the sum of the squared residuals of sigmoid(xdata, *popt) - ydata is minimized. popt are our optimized parameters.\nCODE\nNow we plot our resulting regression model.\nCODE\nPros of Nonlinear Models: 1. Flexibility: Nonlinear models can represent intricate real-world processes that linear models may fail to capture. 2. Realistic Predictions: They often provide more accurate predictions for complex relationships, such as growth phenomena. Cons of Nonlinear Models: 1. Complex Calculus: Estimating parameters in nonlinear models can involve more intricate and computationally intensive methods compared to linear models. In summary, the key distinguishing factor between linear and nonlinear models is the position of parameters within the equations. Linear models have parameters in linear positions, while nonlinear models have parameters in nonlinear positions, allowing them to represent more complex relationships in real-world processes.\nNon-Linear with Linear Regression: Nonlinear regression modeling is akin to linear regression modeling in that both seek to track a particular response from a set of variables graphically. However, nonlinear models are more complex to develop due to the iterative nature of approximations, often involving trial-and-error. Methods like the Gauss-Newton method and the Levenberg-Marquardt method are employed in this process. Identifying Nonlinearity: Regression models that appear nonlinear might be intrinsically linear, and the curve estimation procedure helps identify the nature of functional relationships. Linear regression models can also form curves based on the form of the linear regression equation. Conversely, algebraic transformations can render a nonlinear equation as “intrinsically linear.” Example: One practical example of nonlinear regression is predicting population growth over time. A scatterplot of changing population data may reveal a nonlinear relationship, necessitating the use of a nonlinear regression model. For instance, a logistic population growth model can estimate population for unmeasured periods and predict future growth. Considerations: Variables used in nonlinear regression should be quantitative, and accurate results depend on specifying the relationship between variables accurately. Good starting values are crucial for convergence and obtaining globally optimal solutions. ←code→\nComparison: Linear Regression: Well-suited for linear relationships. Simplicity and interpretability. Limited flexibility in capturing complex patterns. Non-Linear Regression: Accommodates non-linear relationships. Greater flexibility in capturing intricate patterns. May require more data to prevent overfitting. Conclusion: In the diverse landscape of Machine Learning, choosing between Linear and Non-Linear Regression depends on the nature of the data and the underlying relationships. Linear Regression provides a solid foundation for straightforward relationships, while Non-Linear Regression allows for a more intricate exploration of complex patterns. Understanding the strengths and limitations of each technique empowers data scientists to select the right tool for the predictive task at hand, contributing to more accurate and meaningful insights. As we navigate the regression terrain, the synergy of linear and non-linear approaches paves the way for comprehensive modeling and prediction capabilities in the world of Machine Learning.\nReferences : 1.https://www.kaggle.com/code/ibrahimbahbah/non-linear-regression-tutorial/notebook#II.-Non-Linear-Regression-example"
  },
  {
    "objectID": "posts/Blog1/index.html",
    "href": "posts/Blog1/index.html",
    "title": "Blog 1 : Probability theory and random variables",
    "section": "",
    "text": "In the intricate landscape of data analysis, outliers emerge as the unconventional elements that refuse to conform. These peculiar data points, standing out from the crowd, can introduce significant distortions to the overall dataset. Whether stemming from errors, inconsistencies, or simply unique observations, managing outliers becomes a pivotal step in the process of data cleaning and preprocessing. This careful curation ensures that our analytical endeavors are built upon trustworthy and representative data.\nThe Significance of Outlier Detection:\nThink of outlier detection as playing detective with our dataset. During the crucial phases of cleaning and preprocessing, where we address missing values and pinpoint outliers, it’s imperative to contextualize our actions. The decision to remove outliers should be influenced by the specific use case, as blindly discarding data points may not always align with the broader objectives. The ultimate aim is to bolster our model’s performance when faced with new or unseen data. IMAGE\nStrategies for Outlier Detection: IMAGE\n\nUsing Standard Deviation: An approach grounded in setting limits based on standard deviations. Lower Limit: μ - 3σ, Upper Limit: μ + 3σ. Any data point beyond this range is flagged as an outlier. This method relies on the assumption that 99.7% of data falls within three standard deviations.\nUsing Z-score: Standard deviation is a metric of variance, i.e., how much the individual data points are spread out from the mean. In statistics, if a data distribution is approximately normal, then about 68% of the data values lie within one standard deviation of the mean and about 95% are within two standard deviations, and about 99.7% lie within three standard deviations. The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. The intuition behind the Z-score is to describe any data point by finding its relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1, i.e., normal distribution. You must be wondering how this helps in identifying the outliers. While calculating the Z-score, we re-scale and center the data and look for data points that are too far from zero. These data points, which are way too far from zero, will be treated as the outliers. In most cases, a threshold of 3 or -3 is used, i.e., if the Z-score value is greater than or less than 3 or -3, respectively, that data point will be identified as an outlier. This technique assumes a Gaussian distribution of the data. The outliers are the data points that are in the tails of the distribution and therefore far from the mean. How far depends on a set threshold for the normalized data points calculated with the formula:\n\nAn outlier is then a normalized data point that has an absolute value greater than z(thr). That is |z(score)| &gt; z(thr). Commonly used Z(thr) values are 2.5, 3.0, and 3.5. Here we will be using 3.0. For example, I’ll take up the Medical Cost Personal Datasets for explaining the Z-Score method.This technique assumes a Gaussian distribution of the data.\n\nUsing IQR (Interquartile Range):\n\nThe concept of the Interquartile Range (IQR) is used to build the boxplot graphs. IQR is a concept in statistics that is used to measure the statistical dispersion and data variability by dividing the dataset into quartiles.In simple words, any dataset or any set of observations is divided into four defined intervals based upon the values of the data and how they compare to the entire dataset. A quartile is what divides the data into three points and four intervals.It is the difference between the third quartile and the first quartile (IQR = Q3 - Q1). Outliers in this case are defined as the observations that are below (Q1−1.5×IQR) or above (Q3+1.5×IQR) or boxplot lower whisker or above boxplot upper whisker. It can be visually represented by the box plot.\nFocused on the central 50% of data. Outliers are identified outside the range[q25−1.5×IQR,q75+1.5×IQR] 4. Using Percentile:Leveraging percentiles to isolate outliers outside the interquartile range.Outliers are discarded if they stray beyond the confines of [q25−1.5×IQR,q75+1.5×IQR]. In the intricate dance of data analysis, the ability to detect and navigate outliers is akin to mastering a subtle art. Employing techniques such as standard deviation, z-score, IQR, and percentiles empowers us to pinpoint and manage anomalies with finesse. The choice of method is nuanced, guided by the unique characteristics of the dataset and the specific demands of the analysis. By embracing the challenge of outlier detection, we fortify our data’s integrity, ensuring it stands resilient and poised for insightful revelations. More Content: Outliers can have many causes, such as: Measurement or input error. Data corruption. True outlier observation. There is no precise way to define and identify outliers in general because of the specifics of each dataset. Instead, you, or a domain expert, must interpret the raw observations and decide whether a value is an outlier or not. Nevertheless, we can use statistical methods to identify observations that appear to be rare or unlikely given the available data. This does not mean that the values identified are outliers and should be removed. A good tip is to consider plotting the identified outlier values, perhaps in the context of non-outlier values to see if there are any systematic relationships or patterns to the outliers.\nTypes of Outliers:\nOutliers can be of two types:\n\nUnivariate\nMultivariate.\n\nUnivariate outliers can be found when we look at the distribution of a single variable. Multivariate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions. Let us understand this with an example. Let’s say we are understanding the relationship between height and weight. Below, we have univariate and bivariate distribution for Height and Weight. Take a look at the box plot. We do not have any outliers (above and below 1.5 * IQR, a common method). Now look at the scatter plot. Here, we have two values below and one above the average in a specific segment of weight and height.\nImpact of Outliers on a Dataset: Outliers can drastically change the results of data analysis and statistical modeling. There are numerous unfavorable impacts of outliers in the dataset: It increases the error variance and reduces the power of statistical tests. If the outliers are non-randomly distributed, they can decrease normality. They can bias or influence estimates that may be of substantive interest. They can also impact the basic assumption of Regression, ANOVA, and other statistical model assumptions. To understand the impact deeply, let’s take an example to check what happens to a dataset with and without outliers in the dataset.\nCode : For percentile 1. Import all the libraries 2. Load and clean the dataset 3. Finding the percentile 4. Visualization\nFor Standard Deviation Download the dataset from : https://www.kaggle.com/datasets/spscientist/students-performance-in-exams\n\nImport libraries\nRead the dataset\nStandard deviation\nVisualization"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blogs",
    "section": "",
    "text": "Blog 1 : Probability theory and random variables\n\n\n\n\n\n\n\nProbability\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSandhya Vinukonda\n\n\n\n\n\n\n  \n\n\n\n\nBlog 2 : Clustering\n\n\n\n\n\n\n\nMachine Learning\n\n\nUnsupervised Learning\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSandhya Vinukonda\n\n\n\n\n\n\n  \n\n\n\n\nBlog 3 : Linear and nonlinear regression\n\n\n\n\n\n\n\nMachine Learning\n\n\nRegression Analysis\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSandhya Vinukonda\n\n\n\n\n\n\n  \n\n\n\n\nBlog 4 : Classification\n\n\n\n\n\n\n\nMachine Learning\n\n\nSupervised Learning\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSandhya Vinukonda\n\n\n\n\n\n\n  \n\n\n\n\nBlog 5 : Anomaly/Outliers Detection\n\n\n\n\n\n\n\nMachine Learning\n\n\nAnomaly Detection\n\n\nOutlier Analysis\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSandhya Vinukonda\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog2/index.html",
    "href": "posts/Blog2/index.html",
    "title": "Blog 2 : Clustering",
    "section": "",
    "text": "Let’s talk about a fascinating part of machine learning and unsupervised learning. In this world, there’s a standout technique known as clustering, where we work with data that doesn’t come pre-labeled. Instead, we group similar data points into clusters based on common traits or patterns. This blog will take you through the basics of clustering, its types, and zoom in on two popular clustering buddies: K Means and Hierarchical Clustering.\nUnderstanding Clustering:\nClustering is like a detective trying to find hidden connections in a bunch of clues (data points). The cool thing is, no one is telling the detective what to look for – the algorithm figures it out on its own.\n\nTypes of Clustering:\nHard Clustering:\nImagine putting each data point into one exclusive group. That’s hard clustering for you. Classic example: K Means.\nSoft Clustering:\nNow, think of data points being chill and belonging to multiple groups with varying levels of closeness. That’s soft clustering. Fuzzy C-Means is a soft clustering star.\nDifferent Clustering Models:\n\nConnectivity Models: These algorithms look at how close data points are to each other. Example: Hierarchical Clustering.\nCentroid Models: These focus on the center points of clusters. K Means is a big player here.\nDistribution Models: Assume that data points follow a common pattern. Think of Gaussian Mixture Models (GMM).\nDensity Models: These spot clusters based on where data points crowd up. A famous one is DBSCAN.\nProminent Clustering Algorithms: K Means Clustering: It’s like dividing data into clubs based on the average characteristics of each group. Simple and efficient – a go-to for many.\nHierarchical Clustering: This one builds a family tree of clusters, like tracing your ancestry. It’s cool because it allows for nested or overlapping groups.\nDifference Between K Means and Hierarchical Clustering: Here’s the lowdown on how K Means and Hierarchical Clustering differ:\nNature of Clusters: K Means makes clear-cut groups with no sharing allowed. Hierarchical Clustering creates a family tree where groups can overlap or nest.\nNumber of Clusters: K Means needs to know how many groups you want before it starts. Hierarchical Clustering is more flexible – it generates a tree, and you can decide how many groups later.\nComputation Complexity: K Means is quicker with the calculations, good for big datasets. Hierarchical Clustering can take more time, especially with lots of data. Conclusion: In the unsupervised learning world, clustering is like a superhero revealing hidden patterns in data without anyone giving it a roadmap. Knowing the ins and outs of clustering, the different types, and the quirks of K Means and Hierarchical Clustering gives you a powerful toolkit for finding the stories hidden in your data. As technology races forward, the possibilities for unsupervised learning and clustering are endless, promising discoveries in fields from healthcare to finance.\nLet’s Start Coding\n\n\nImport Libraries\nRead data and clean it\nModel : K Means"
  },
  {
    "objectID": "posts/Blog4/index.html",
    "href": "posts/Blog4/index.html",
    "title": "Blog 4 : Classification",
    "section": "",
    "text": "Classification : The primary objective in supervised learning is to teach the model to predict the correct label or outcome for a given input. The model learns patterns and relationships in the training data, associating specific features with their corresponding labels.The training process involves exposing the model to the labeled training data, allowing it to adjust its internal parameters to make accurate predictions. Once trained, the model is then tested on new, unseen data (test data) to evaluate its generalization capabilities.The effectiveness of the supervised learning method is assessed by measuring its accuracy on the test data. Accuracy is calculated by comparing the model’s predictions with the actual labels in the test set. This provides insights into how well the model performs on previously unseen examples.\n\nIn supervised learning, where we already know the correct answers, there are two main flavors: classification and regression. The key factor in deciding which one to use is the nature of the label data. If your label data consists of continuous values, then it’s a regression task. Take predicting house prices as an example – here, the goal is to estimate a continuous value based on features like square footage, location, and the number of bedrooms.On the flip side, if your task involves predicting categorical outcomes, like whether a student will be accepted into a specific university or not, then it’s a classification problem. In this scenario, the label data has distinct categories (admitted or not admitted), making it suitable for a classification approach.So, whether you’re dealing with predicting prices or university admissions, understanding if your label data is continuous or categorical guides you in choosing between regression and classification for your supervised learning journey.\nEager Learners: Eager learners proactively build a model from a training dataset before making predictions. They invest more time during the training process to generalize better by learning the weights and relationships within the data. Once the model is trained, making predictions is relatively quicker. Examples: Logistic Regression: A widely-used algorithm for binary and multiclass classification. It models the probability of a certain class. Support Vector Machine (SVM): Efficiently classifies data points by finding the optimal hyperplane that separates different classes. Decision Trees: Tree-like models that make decisions based on features at each node, suitable for classification and regression. Artificial Neural Networks (ANN): Complex models inspired by the human brain, composed of interconnected nodes that process information in layers.\nLazy Learners (Instance-Based Learners): Characteristics:\nLazy learners don’t construct a model immediately; they memorize the training data. During prediction, they dynamically search for the nearest neighbor from the entire training dataset. This approach can be slower during prediction but adapts well to changes in the dataset. Examples:\nK-Nearest Neighbor (KNN): Classifies data points based on the majority class of their k-nearest neighbors. Case-Based Reasoning: Makes predictions based on past cases, comparing the current problem to previously solved ones. Understanding these distinctions helps in choosing the right algorithm based on the characteristics of the dataset and the specific requirements of the problem at hand. Eager and lazy learners offer different trade-offs in terms of training time, prediction speed, and adaptability to changes in the dataset.\n\nTypes of Classifications : 1. Binary : 0/1, spam and not spam, yes and no, negative and positive. Example : (long) 2. Multi-class classification Example (one-vs-rest and one-vs-one) 3. Multi-label classification 4. Imbalanced classification\nLet’s get to coding!\nClassification takes place when the target variable is discrete. When a target variable isn’t discrete, then regression takes place for the continuous varible.\nTypes of classification in ML :\nLet’s Start Coding\n\nLet’s look at a Machine Learning code example that shows binary classification of Blood Transfusion Service Centre.\nBefore we start coding let’s download the data and understand it. Go to link : https://archive.ics.uci.edu/dataset/176/blood+transfusion+service+center and click on download. About Data : This study focuses on the critical role of blood transfusions in saving lives, addressing the challenges of maintaining an adequate blood supply for medical needs. The research employs the RFMTC marketing model, a modified version of RFM, using the donor database of the Blood Transfusion Service Center in Hsin-Chu City, Taiwan. The study randomly selects 748 donors and gathers data on Recency, Frequency, Monetary contribution, Time, and a binary variable indicating blood donation in March 2007. This dataset forms the basis for building an RFMTC model to enhance understanding and prediction in blood donation patterns.\nLet’s start coding : 1. Import Libraries\ncode\n\nRead data and clean it\n\ncode\n\nVisualization of the data\n\ncode\n\nSplitting data into train and test\n\ncode\n\nModel : Logistic Regression\n\ncode\n\nEvaluate the model\n\ncode"
  }
]