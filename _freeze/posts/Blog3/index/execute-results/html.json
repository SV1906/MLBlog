{
  "hash": "6cb776571e6a7f98074d29303222213b",
  "result": {
    "markdown": "---\ntitle: 'Blog 3 :  Linear and nonlinear regression '\nauthor: Sandhya Vinukonda\ndate: '2023-12-06'\ncategories:\n  - Machine Learning\n  - Regression Analysis\n---\n\nIntroduction:\nIn the dynamic world of Machine Learning, where predictions and decisions guide intelligent systems, Regression emerges as a powerful technique for understanding relationships between variables. In this blog, we embark on a journey through the realms of Linear and Non-Linear Regression, exploring their nuances, applications, and the impact they can have on predicting future outcomes.\nLinear Models:\nLinear models are characterized by having parameters in linear positions within the equation. The general form of a linear model is expressed as y=a+bx. Here, y is the dependent variable, x is the independent variable, and a and b are the parameters. Linear models can also include additional terms such as cx^2 or b/x, as long as the parameters are in linear positions.\nExamples of Linear Models:\ny = a+bx  line \ny = a + bx + cx^2\ny = a + (b/x)\nLet's start coding :  \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n# First line of plots\nplt.figure(figsize=(15, 5))\n# Plot 1\nplt.subplot(1, 3, 1)\nx1 = np.arange(-6.0, 6.0, 0.1)\ny1 = 3 * x1 + 2\ny1_noise = 2 * np.random.normal(size=x1.size)\ny1_data = y1 + y1_noise\nplt.scatter(x1, y1_data, color='blue', alpha=0.7, label='Data Points')\nplt.plot(x1, y1, color='red', label='Regression Line')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.title('Linear Regression')\nplt.legend()\n\n# Plot 2\nplt.subplot(1, 3, 2)\nx2 = np.arange(-6.0, 6.0, 0.1)\ny2 = 1 * (x2**3) + 2 * (x2**2) + 1 * x2 + 3\ny2_noise = 20 * np.random.normal(size=x2.size)\ny2_data = y2 + y2_noise\nplt.plot(x2, y2_data, 'bo')\nplt.plot(x2, y2, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.title('Polynomial Regression')\n\n# Plot 3\nplt.subplot(1, 3, 3)\nx3 = np.arange(-6.0, 6.0, 0.1)\ny3 = np.power(x3, 2)\ny3_noise = 2 * np.random.normal(size=x3.size)\ny3_data = y3 + y3_noise\nplt.plot(x3, y3_data, 'bo')\nplt.plot(x3, y3, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.title('Quadratic Regression')\n\n# Show the first line of plots\nplt.show()\n# Second line of plots\nplt.figure(figsize=(15, 5))\n# Plot 4\nplt.subplot(1, 3, 1)\nx4 = np.arange(-6.0, 6.0, 0.1)\ny4 = np.exp(x4)\nplt.plot(x4, y4, label='Exponential Curve')\nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.title('Exponential Regression')\nplt.legend()\n\n# Plot 5\nplt.subplot(1, 3, 2)\nx5 = np.arange(1.0, 10.0, 0.1)\ny5 = np.log(x5)\nplt.plot(x5, y5)\nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.title('Logarithmic Regression')\n\n# Plot 6\nplt.subplot(1, 3, 3)\nx6 = np.arange(-5.0, 5.0, 0.1)\ny6 = 1 - 4 / (1 + np.power(3, x6 - 2))\nplt.plot(x6, y6)\nplt.ylabel('Dependent Variable')\nplt.xlabel('Independent Variable')\nplt.title('Sigmoidal/Logistic Regression')\n\n# Show the second line of plots\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=1191 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=1188 height=449}\n:::\n:::\n\n\nNonlinear Models:\nNonlinear models are characterized by having parameters in nonlinear positions within the equation. The general form of a nonlinear model is expressed as : y=f(X,β)+ε where f is a nonlinear function of the independent variable(s) X and parameters β.\nNonlinear models capture more complex relationships, allowing for curves, exponentials, logarithms, and other nonlinear patterns.\nExamples of Nonlinear Models:\n1. y = zx^b \n2. y = ae^(bx)\n3. y = ae^(k/x)\n\nCode : \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\npath='china_gdp.csv'\ndf = pd.read_csv(path)\ndf.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1960</td>\n      <td>5.918412e+10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1961</td>\n      <td>4.955705e+10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1962</td>\n      <td>4.668518e+10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1963</td>\n      <td>5.009730e+10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1964</td>\n      <td>5.906225e+10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1965</td>\n      <td>6.970915e+10</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1966</td>\n      <td>7.587943e+10</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1967</td>\n      <td>7.205703e+10</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1968</td>\n      <td>6.999350e+10</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1969</td>\n      <td>7.871882e+10</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nPlotting the Dataset\nThis is what the datapoints look like. It kind of looks like an either logistic or exponential function. The growth starts off slow, then from 2005 on forward, the growth is very significant. And finally, it decelerate slightly in the 2010s.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nplt.figure(figsize=(8,5))\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nplt.plot(x_data, y_data, 'ro')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=663 height=443}\n:::\n:::\n\n\nChoosing a model\nFrom an initial look at the plot, we determine that the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nX = np.arange(-5,5.0, 0.1)\nY = 1.0 / (1.0 + np.exp(-X))\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=589 height=429}\n:::\n:::\n\n\nThe formula for the logistic function is the following:\nIMAGE\nβ1: Controls the curve's steepness,\nβ2: Slides the curve on the x-axis.\nBuilding The Model\nNow, let's build our regression model and initialize its parameters.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n```\n:::\n\n\nLets look at a sample sigmoid line that might fit with the data:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nbeta_1 = 0.10\nbeta_2 = 1990.0\n\n#logistic function\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n#plot initial prediction against datapoints\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=571 height=425}\n:::\n:::\n\n\nOur task here is to find the best parameters for our model. Lets first normalize our x and y:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Lets normalize our data\nxdata =x_data/max(x_data)\nydata =y_data/max(y_data)\n```\n:::\n\n\nHow we find the best parameters for our fit line?\nwe can use curve_fit which uses non-linear least squares to fit our sigmoid function, to data. Optimal values for the parameters so that the sum of the squared residuals of sigmoid(xdata, *popt) - ydata is minimized.\npopt are our optimized parameters.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n beta_1 = 690.451712, beta_2 = 0.997207\n```\n:::\n:::\n\n\nNow we plot our resulting regression model.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nx = np.linspace(1960, 2015, 55)\nx = x/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=663 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# split data into train/test\nmsk = np.random.rand(len(df)) < 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , test_y) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean absolute error: 0.02\nResidual sum of squares (MSE): 0.00\nR2-score: 0.97\n```\n:::\n:::\n\n\nPros of Nonlinear Models:\n1. Flexibility: Nonlinear models can represent intricate real-world processes that linear models may fail to capture.\n2. Realistic Predictions: They often provide more accurate predictions for complex relationships, such as growth phenomena.\nCons of Nonlinear Models:\n1. Complex Calculus: Estimating parameters in nonlinear models can involve more intricate and computationally intensive methods compared to linear models.\nIn summary, the key distinguishing factor between linear and nonlinear models is the position of parameters within the equations. Linear models have parameters in linear positions, while nonlinear models have parameters in nonlinear positions, allowing them to represent more complex relationships in real-world processes.\n\nNon-Linear with Linear Regression:\nNonlinear regression modeling is akin to linear regression modeling in that both seek to track a particular response from a set of variables graphically. However, nonlinear models are more complex to develop due to the iterative nature of approximations, often involving trial-and-error. Methods like the Gauss-Newton method and the Levenberg-Marquardt method are employed in this process.\nIdentifying Nonlinearity:\nRegression models that appear nonlinear might be intrinsically linear, and the curve estimation procedure helps identify the nature of functional relationships. Linear regression models can also form curves based on the form of the linear regression equation. Conversely, algebraic transformations can render a nonlinear equation as \"intrinsically linear.\"\nExample:\nOne practical example of nonlinear regression is predicting population growth over time. A scatterplot of changing population data may reveal a nonlinear relationship, necessitating the use of a nonlinear regression model. For instance, a logistic population growth model can estimate population for unmeasured periods and predict future growth.\nConsiderations:\nVariables used in nonlinear regression should be quantitative, and accurate results depend on specifying the relationship between variables accurately. Good starting values are crucial for convergence and obtaining globally optimal solutions.\n←code→ \n\nComparison:\nLinear Regression:\nWell-suited for linear relationships.\nSimplicity and interpretability.\nLimited flexibility in capturing complex patterns.\nNon-Linear Regression:\nAccommodates non-linear relationships.\nGreater flexibility in capturing intricate patterns.\nMay require more data to prevent overfitting.\nConclusion:\nIn the diverse landscape of Machine Learning, choosing between Linear and Non-Linear Regression depends on the nature of the data and the underlying relationships. Linear Regression provides a solid foundation for straightforward relationships, while Non-Linear Regression allows for a more intricate exploration of complex patterns. Understanding the strengths and limitations of each technique empowers data scientists to select the right tool for the predictive task at hand, contributing to more accurate and meaningful insights. As we navigate the regression terrain, the synergy of linear and non-linear approaches paves the way for comprehensive modeling and prediction capabilities in the world of Machine Learning.\n\nReferences : \n1.https://www.kaggle.com/code/ibrahimbahbah/non-linear-regression-tutorial/notebook#II.-Non-Linear-Regression-example\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}