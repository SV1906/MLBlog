{
  "hash": "498031fdcb888c760da5607082515b82",
  "result": {
    "markdown": "---\ntitle: 'Blog 2 : Clustering '\nauthor: Sandhya Vinukonda\ndate: '2023-12-06'\ncategories:\n  - Machine Learning\n  - Unsupervised Learning\n---\n\nLet's talk about a fascinating part of machine learning and unsupervised learning. In this world, there's a standout technique known as clustering, where we work with data that doesn't come pre-labeled. Instead, we group similar data points into clusters based on common traits or patterns. This blog will take you through the basics of clustering, its types, and zoom in on two popular clustering buddies: K Means and Hierarchical Clustering.\n\nUnderstanding Clustering:\n\nClustering is like a detective trying to find hidden connections in a bunch of clues (data points). The cool thing is, no one is telling the detective what to look for – the algorithm figures it out on its own.\n\n![](Clustering_Soft_Hard.png)\n\nTypes of Clustering:\n\nHard Clustering:\n\nImagine putting each data point into one exclusive group. That's hard clustering for you.\nClassic example: K Means.\n\nSoft Clustering:\n\nNow, think of data points being chill and belonging to multiple groups with varying levels of closeness. That's soft clustering.\nFuzzy C-Means is a soft clustering star.\n\nDifferent Clustering Models:\n \n ![](Clustering_models.png)\n\nConnectivity Models:\nThese algorithms look at how close data points are to each other.\nExample: Hierarchical Clustering.\n\nCentroid Models:\nThese focus on the center points of clusters.\nK Means is a big player here.\n\nDistribution Models:\nAssume that data points follow a common pattern.\nThink of Gaussian Mixture Models (GMM).\n\nDensity Models:\nThese spot clusters based on where data points crowd up.\nA famous one is DBSCAN.\n\nProminent Clustering Algorithms:\nK Means Clustering:\nIt's like dividing data into clubs based on the average characteristics of each group.\nSimple and efficient – a go-to for many.\n\nHierarchical Clustering:\nThis one builds a family tree of clusters, like tracing your ancestry.\nIt's cool because it allows for nested or overlapping groups.\n\nDifference Between K Means and Hierarchical Clustering:\nHere's the lowdown on how K Means and Hierarchical Clustering differ:\n\nNature of Clusters:\nK Means makes clear-cut groups with no sharing allowed.\nHierarchical Clustering creates a family tree where groups can overlap or nest.\n\nNumber of Clusters:\nK Means needs to know how many groups you want before it starts.\nHierarchical Clustering is more flexible – it generates a tree, and you can decide how many groups later.\n\nComputation Complexity:\nK Means is quicker with the calculations, good for big datasets.\nHierarchical Clustering can take more time, especially with lots of data.\nConclusion:\nIn the unsupervised learning world, clustering is like a superhero revealing hidden patterns in data without anyone giving it a roadmap. Knowing the ins and outs of clustering, the different types, and the quirks of K Means and Hierarchical Clustering gives you a powerful toolkit for finding the stories hidden in your data. As technology races forward, the possibilities for unsupervised learning and clustering are endless, promising discoveries in fields from healthcare to finance.\n\nLet’s Start Coding \n\n![](KMeans_StepsOfCode.png)\n\n1. Import Libraries \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\n```\n:::\n\n\n2. Read data and clean it \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Load the Iris dataset\niris = datasets.load_iris()\ndata = iris.data\nfeatures = iris.feature_names\ntarget = iris.target\n\n# Check for missing values\nif np.any(np.isnan(data)):\n    print(\"Warning: Missing values found in the dataset.\")\n    # Handle missing values as needed\n\n# Standardize the features (important for K-means)\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# Check data types\nprint(\"Data Types:\", data_scaled.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData Types: float64\n```\n:::\n:::\n\n\n3. Model : K Means \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Choose the number of clusters (K) - for the Iris dataset, K=3 makes sense due to the three classes\nk = 3\n\n# Apply K-means clustering\nkmeans = KMeans(n_clusters=k, random_state=42)\nkmeans.fit(data_scaled)\n\n# Get the cluster labels and centroids\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\n# Add cluster labels to the original Iris dataset\niris_df = pd.DataFrame(data_scaled, columns=features)\niris_df['Cluster'] = labels\n\n# Visualize the clusters (using only the first two features for simplicity)\nplt.scatter(iris_df.iloc[:, 0], iris_df.iloc[:, 1], c=labels, cmap='viridis', edgecolors='k', s=50)\nplt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title('K-means Clustering on Iris Dataset')\nplt.xlabel(features[0])\nplt.ylabel(features[1])\nplt.legend()\nplt.show()\n\n# Evaluate the clustering using silhouette score\nsilhouette_avg = silhouette_score(data_scaled, labels)\nprint(f\"Silhouette Score: {silhouette_avg}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\gayat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=587 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette Score: 0.45994823920518635\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}